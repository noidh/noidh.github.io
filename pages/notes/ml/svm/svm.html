<!DOCTYPE html>
<html>
    <head>
        <title>Support Vector Machine</title>
        <link rel="stylesheet" href="../../../../index_css.css">
		
		<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
		<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.1/es5/tex-mml-chtml.js"></script>
    </head>

    <body>
        <div class="header">
			<div  style="width: 270px; float: left;">
				<h2 style="display: flex; align-self: left; position:absolute; top:0px;">Doan Huu Moi (MEO)</h2>
				<h4></h4><p><a><< doanhuunoi@gmail.com >></a></p></h4>
				<p style="word-wrap: break-word;">I'm an enthusiastic researcher in Image Processing and Computer Vision. Learning new theories and bringing them to practice is my hobby. My research topics are Pattern Matching, Stereo-Vision Reconstruction, and other Miscellaneous.</p>
			</div>
			<div style="float: left;">
				<img style="height: 220px;" src="https://noidh.github.io/header.jpg">
			</div>
        </div>
        <div class="navigation">
            <div class="nav_item"><a href="https://noidh.github.io">[Home]</a></div>
        </div>

        <div class="introduction">	
			<h3>1. Support Vector Machine</h3>
			<p>Support Vector Machine(SVM) is one of the most popular machine learning algorithms used for classifying task.</p>
			<p>In binary classification, SVM finds the best decision boundary that separates two classes with maximum margin distance. This means the distance between nearest data points of 2 classes is as large as possible</p>
			<img class="image_center" src="svm.png">
			<p>In which, support vectors are the data points that lie closest to the decision boundary. In other words, support vectors are critical points that influence to the direction and position of the decision boundary.</p>
			<img class="image_center" src="svm_1.bmp">	

			<p>Suppose there are an input feature vector \(X=\left[\begin{array}{}x_1&x_2&...&x_N\end{array}\right]\), an output value \(Y\), output is vector \(W=\left[\begin{array}{}w_1&w_2&...&w_N\end{array}\right]\), so that the linear combination \(WX + b = Y\) or \(\sum_{i=1}^{N} w_ix_i + b = Y\)</p>
			<img class="image_center" src="svm_2.bmp">

			<p>Define the hyperplane \(H\) such that \(WX_i + b \ge +1\) when \(Y_i = +1\) and \(WX_i + b \le -1\) when \(Y_i = -1\)</p>
			<p>The plane \(H_1\) is \(WX_i+b=+1\), \(H_2\) is \(WX_i + b =-1\), \(H_0\) is \(WX_i + b = 0\)</p>
			<p>The value \(d+\) is the distance from the nearest positive point to \(H_0\), \(d-\) is the distance from the nearest negative point to \(H_0\)</p>
            <p>The points on \(H_1\) and \(H_2\) are support vectors.</p>
			<p>The margin of hyperplane is the distance between \(H_1\) and \(H_2\) or the summation of \(d+\) and \(d-\)</p>

			<p>The distance from any point on \(H_1\) to \(H_0\) is \(\frac{\left|WX + b\right|}{||W||} = \frac{1}{||W||}\), then the distance between \(H_1\) and \(H_2\) is \(\frac{2}{||W||}\)</p>
			
			<p>In order to maximize the margin, we need to minimize \(||W||\) with constraints \(WX_i + b \ge +1\) if \(Y_i = +1\) and \(WX_i + b \le -1\) if \(Y_i = -1\).</p>
			<p>The constraints can be recombined as \(Y_i(WX_i) \ge 1\)</p>

			<p>So, the SVM problem is minimizing \(||W||\) or \(\frac{1}{2} ||W||^2\) such that \(Y_i(WX_i + b) = 1\) or \(Y_i(WX_i + b) - 1 = 0\). This optimization problem with contraint can be solved by <a href="https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/constrained-optimization/a/lagrange-multipliers-single-constraint">the Lagrange multiplier</a>.</p>
			<p>\[min L = \frac{1}{2}||W||^2 - \sum_{i=1}^M \lambda_i(Y_i(WX_i + b) - 1)\]</p>
			<p>\[\Rightarrow min L = \frac{1}{2}||W||^2 - \sum_{i=1}^M \lambda_iY_i(WX_i + b) + \sum_{i=1}^M \lambda_i\tag{1}\]</p>
			<p>Where \(M\) is number of observations.</p>
			<p>Take the partial derivative with respect to \(W\) and \(b\) and set them to zero</p>
			<p>\[\frac{\delta L}{\delta W} = ||W|| - \sum_{i=1}^M \lambda_iY_iX_i = 0\]</p>
			<p>\[\frac{\delta L}{\delta b} = \sum_{i=1}^M \lambda_iY_i = 0\]</p>
			<p>\[\Rightarrow ||W|| = \sum_{i=1}^M \lambda_iY_iX_i, \sum_{i=1}^M \lambda_iY_i = 0\tag{2}\]</p>

			<p>Substitute \((2)\) into \((1)\), we have a new Lagrange Dual Problem that is instead of minimizing \(L\) wrt \(W, b\), we maximize \(L\) wrt \(\lambda\) (<a href="https://web.mit.edu/6.034/wwwbob/svm.pdf">An Idiot’s guide to Support vector machines (SVMs)</a>)</p>
			<p>\[max L = \frac{1}{2}\sum_{i=1}^M \lambda_iY_iX_i \sum_{j=1}^M \lambda_jY_jX_j - \sum_{i=1}^M \lambda_iY_iX_i\sum_{j=1}^M \lambda_jY_jX_j - b \sum_{i=1}^M \lambda_iY_i + \sum_{i=1}^M \lambda_i\]</p>
			<p>\[\Rightarrow max L = \frac{1}{2}\sum_{i=1}^M \sum_{j=1}^M \lambda_iY_iX_i \lambda_jY_jX_j - \sum_{i=1}^M \sum_{j=1}^M \lambda_iY_iX_i \lambda_jY_jX_j + \sum_{i=1}^M \lambda_i\]</p>
			<p>\[\Rightarrow max L = \sum_{i=1}^M \lambda_i - \frac{1}{2}\sum_{i=1}^M \sum_{j=1}^M \lambda_i\lambda_jY_iY_jX_iX_j, st. \sum_{i=1}^M \lambda_i Y_i = 0 \tag{3}\]</p>

			<p>Solve the quadratic equation \((3)\) to obtain \(\lambda\), then substitute \(\lambda\) back into equation \((1)\) and solve it to obtain \(W\) and \(b\)</p>
			<h3>2. Experiment</h3>
			<p>Demo video</p>
			

			<h3>3. References</h3>
			<p>
				<ul>
					<li><a href="https://web.mit.edu/6.034/wwwbob/svm.pdf">An Idiot’s guide to Support vector machines (SVMs)</a></li>
					<li>https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47</li>
					<li>https://towardsdatascience.com/support-vector-machine-explained-8bfef2f17e71</li>
                    <li>https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/constrained-optimization/a/lagrange-multipliers-single-constraint</li>
                </ul>
			</p>
		</div>
    </body>
</html>