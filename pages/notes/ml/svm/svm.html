<!DOCTYPE html>
<html lang="en-US">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SVM — Doan Huu Noi</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=DM+Sans:ital,opsz,wght@0,9..40,400;0,9..40,500;0,9..40,600;0,9..40,700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../../../../index_css.css">
    <script src="../../../../index.js"></script>

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.1/es5/tex-mml-chtml.js"></script>

</head>
<body>
    <div id="header" class="site-header"></div>
    <script>Header("header");</script>

    <div class="layout">
        <aside class="sidebar" aria-label="Site navigation">
            <nav class="nav">
                <div class="nav_item">
                    <a href="../../../../index.html" class="nav_link">Home</a>
                </div>
                <div class="nav_item nav_item--dropdown">
                    <a href="../../../../project.html" class="nav_link">Personal Projects</a>
                    <ul class="nav_dropdown">
                        <li><a href="../../../../pages/sw/ximagetool/ximagetool.html">XImage Tool</a></li>
                        <li><a href="../../../../pages/sw/ximage2text/ximagetotext.html">XText</a></li>
                    </ul>
                </div>
                <div class="nav_item nav_item--dropdown">
                    <a href="../../../../blog.html" class="nav_link nav_link--current">Technical Blog</a>
                    <ul class="nav_dropdown">
                        <li><a href="../../../../notes_ip.html">Image Processing</a></li>
                        <li><a href="../../../../notes_cv.html">Computer Vision</a></li>
                        <li><a href="../../../../notes_ml.html">Machine Learning</a></li>
                        <li><a href="../../../../opengl_notes.html">3D Rendering</a></li>
                        <li><a href="../../../../notes_miscellaneous.html">Miscellaneous</a></li>
                    </ul>
                </div>
            </nav>
        </aside>

        <main class="content">
            <section class="section page-prose">
			<h3>1. Support Vector Machine</h3>
			<p>Support vector machines (SVMs) are among the most popular machine learning algorithms used for classification tasks.</p>
			<p>In binary classification, SVM finds the best decision boundary that separates two classes with maximum margin distance. This means the distance between the nearest data points of the two classes is as large as possible</p>
			<img class="image_center" src="svm.png">
			<p>In this setting, support vectors are the data points that lie closest to the decision boundary. In other words, support vectors are critical points that influence the direction and position of the decision boundary.</p>
			<img class="image_center" src="svm_1.bmp">	

			<p>Suppose there is an input feature vector \(X=\left[\begin{array}{}x_1&x_2&...&x_N\end{array}\right]\), an output value \(Y\), and a weight vector \(W=\left[\begin{array}{}w_1&w_2&...&w_N\end{array}\right]\), so that the linear combination \(WX + b = Y\) or \(\sum_{i=1}^{N} w_ix_i + b = Y\)</p>
			<img class="image_center" src="svm_2.bmp">

			<p>Define the hyperplane \(H\) such that \(WX_i + b \ge +1\) when \(Y_i = +1\) and \(WX_i + b \le -1\) when \(Y_i = -1\)</p>
			<p>The plane \(H_1\) is \(WX_i+b=+1\), \(H_2\) is \(WX_i + b =-1\), \(H_0\) is \(WX_i + b = 0\)</p>
			<p>The value \(d^+\) is the distance from the nearest positive point to \(H_0\), and \(d^-\) is the distance from the nearest negative point to \(H_0\)</p>
            <p>The points on \(H_1\) and \(H_2\) are support vectors.</p>
			<p>The margin of the hyperplane is the distance between \(H_1\) and \(H_2\), or the sum of \(d^+\) and \(d^-\)</p>

			<p>The distance from any point on \(H_1\) to \(H_0\) is \(\frac{\left|WX + b\right|}{||W||} = \frac{1}{||W||}\), then the distance between \(H_1\) and \(H_2\) is \(\frac{2}{||W||}\)</p>
			
			<p>In order to maximize the margin, we need to minimize \(||W||\) with constraints \(WX_i + b \ge +1\) if \(Y_i = +1\) and \(WX_i + b \le -1\) if \(Y_i = -1\).</p>
			<p>The constraints can be recombined as \(Y_i(WX_i) \ge 1\)</p>

			<p>So, the SVM problem is minimizing \(||W||\) or \(\frac{1}{2} ||W||^2\) such that \(Y_i(WX_i + b) = 1\) or \(Y_i(WX_i + b) - 1 = 0\). This constrained optimization problem can be solved by <a href="https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/constrained-optimization/a/lagrange-multipliers-single-constraint">the Lagrange multiplier</a>.</p>
			<p>\[min L = \frac{1}{2}||W||^2 - \sum_{i=1}^M \lambda_i(Y_i(WX_i + b) - 1)\]</p>
			<p>\[\Rightarrow min L = \frac{1}{2}||W||^2 - \sum_{i=1}^M \lambda_iY_i(WX_i + b) + \sum_{i=1}^M \lambda_i\tag{1}\]</p>
			<p>Where \(M\) is number of observations.</p>
			<p>Take the partial derivatives with respect to \(W\) and \(b\) and set them to zero</p>
			<p>\[\frac{\delta L}{\delta W} = ||W|| - \sum_{i=1}^M \lambda_iY_iX_i = 0\]</p>
			<p>\[\frac{\delta L}{\delta b} = \sum_{i=1}^M \lambda_iY_i = 0\]</p>
			<p>\[\Rightarrow ||W|| = \sum_{i=1}^M \lambda_iY_iX_i, \sum_{i=1}^M \lambda_iY_i = 0\tag{2}\]</p>

			<p>Substitute \((2)\) into \((1)\), we have a new Lagrange Dual Problem that is instead of minimizing \(L\) wrt \(W, b\), we maximize \(L\) wrt \(\lambda\) (<a href="https://web.mit.edu/6.034/wwwbob/svm.pdf">An Idiot’s guide to Support vector machines (SVMs)</a>)</p>
			<p>\[max L = \frac{1}{2}\sum_{i=1}^M \lambda_iY_iX_i \sum_{j=1}^M \lambda_jY_jX_j - \sum_{i=1}^M \lambda_iY_iX_i\sum_{j=1}^M \lambda_jY_jX_j - b \sum_{i=1}^M \lambda_iY_i + \sum_{i=1}^M \lambda_i\]</p>
			<p>\[\Rightarrow max L = \frac{1}{2}\sum_{i=1}^M \sum_{j=1}^M \lambda_iY_iX_i \lambda_jY_jX_j - \sum_{i=1}^M \sum_{j=1}^M \lambda_iY_iX_i \lambda_jY_jX_j + \sum_{i=1}^M \lambda_i\]</p>
			<p>\[\Rightarrow max L = \sum_{i=1}^M \lambda_i - \frac{1}{2}\sum_{i=1}^M \sum_{j=1}^M \lambda_i\lambda_jY_iY_jX_iX_j, st. \sum_{i=1}^M \lambda_i Y_i = 0 \tag{3}\]</p>

			<p>Solve the quadratic equation \((3)\) to obtain \(\lambda\), then substitute \(\lambda\) back into equation \((1)\) and solve it to obtain \(W\) and \(b\)</p>
			<h3>2. Experiment</h3>
			<p>Demo video</p>
			

			<h3>3. References</h3>
			<p>
				<ul>
					<li><a href="https://nianlonggu.com/2019/05/24/tutorial-on-SVM/">An Introduction to Support Vector Machines (SVM): Gradient Descent Solution</a></li>
					<li>https://towardsdatascience.com/support-vector-machines-dual-formulation-quadratic-programming-sequential-minimal-optimization-57f4387ce4dd</li>
					<li>https://blog.dominodatalab.com/fitting-support-vector-machines-quadratic-programming</li>
					<li><a href="https://web.mit.edu/6.034/wwwbob/svm.pdf">An Idiot’s guide to Support vector machines (SVMs)</a></li>
					<li>https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47</li>
					<li>https://towardsdatascience.com/support-vector-machine-explained-8bfef2f17e71</li>
                    <li>https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/constrained-optimization/a/lagrange-multipliers-single-constraint</li>
					<li><a href="https://www.robots.ox.ac.uk/~az/lectures/ml/lect2.pdf">The SVM classifier</a></li>
				</ul>
			</p>
            </section>
        </main>
    </div>
</body>
</html>
