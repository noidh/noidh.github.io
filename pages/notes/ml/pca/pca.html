<!DOCTYPE html>
<html lang="en-US">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PCA — Doan Huu Noi</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=DM+Sans:ital,opsz,wght@0,9..40,400;0,9..40,500;0,9..40,600;0,9..40,700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../../../../index_css.css">
    <script src="../../../../index.js"></script>

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.1/es5/tex-mml-chtml.js"></script>

</head>
<body>
    <div id="header" class="site-header"></div>
    <script>Header("header");</script>

    <header class="site-nav" aria-label="Site navigation">
        <nav class="nav nav--horizontal">
            <div class="nav_item">
                <a href="../../../../index.html" class="nav_link">Home</a>
            </div>
            <div class="nav_item nav_item--dropdown">
                <a href="../../../../project.html" class="nav_link">Personal Projects ▾</a>
                <ul class="nav_dropdown">
                    <li><a href="../../../../pages/sw/ximagetool/ximagetool.html">XImage Tool</a></li>
                    <li><a href="../../../../pages/sw/ximage2text/ximagetotext.html">XText</a></li>
                </ul>
            </div>
            <div class="nav_item nav_item--dropdown">
                <a href="../../../../blog.html" class="nav_link nav_link--current">Technical Blog ▾</a>
                <ul class="nav_dropdown">
                    <li><a href="../../../../notes_ip.html">Image Processing</a></li>
                    <li><a href="../../../../notes_cv.html">Computer Vision</a></li>
                    <li><a href="../../../../notes_ml.html">Machine Learning</a></li>
                    <li><a href="../../../../opengl_notes.html">3D Rendering</a></li>
                    <li><a href="../../../../notes_miscellaneous.html">Miscellaneous</a></li>
                </ul>
            </div>
        </nav>
    </header>

    <div class="layout">
        <main class="content">
            <section class="section page-prose">
			<h3>1. Principal Component Analysis</h3>
			<p>Principal component analysis (PCA) is an important statistical technique widely used in machine learning to reduce the dimensionality of a dataset.</p>
            <p>In other words, PCA helps us eliminate unimportant information or noise and keep the most important components in the data.</p>
            <p>Below are the steps for calculating PCA on 2D data, which can be easily extended to higher-dimensional datasets.</p>
            <ol>
                <li>
                    <p>Creating a matrix \(D\) by assembling all the data points into it where each column is one data point.</p>
                    <p>\[D=\left[\begin{array}{} x_1 & x_2 & ... & x_n \\ y_1 & y_2 & ... & y_n \end{array} \right]\]</p>
                </li>

                <li>
                    <p>Calculating the mean of each row of matrix \(D\)</p>
                    <p>\[\mu_x = \frac{1}{n}\sum_{i=1}^n x_i\]</p>
                    <p>\[\mu_y = \frac{1}{n}\sum_{i=1}^n y_i\]</p>
                </li>

                <li>
                    <p>Creating a matrix \(M\) by subtracting its values by the corresponding mean value: this step standardizes the range of the variables. Since variables with larger ranges can potentially dominate the output of the calculation and lead to poor results, we can obtain a better result by dividing by the standard deviation after subtracting the mean value.</p>    
                    <p><p>\[M=\left[\begin{array}{} (x_1 - \mu_x) & (x_2 - \mu_x) & ... & (x_n - \mu_x) \\ (y_1 - \mu_y) & (y_2 - \mu_y) & ... & (y_n - \mu_y) \end{array} \right]\]</p></p>
                </li>

                <li>
                    <p>Calculating the covariance matrix \(C\): the reason we calculate the covariance matrix is to explore the relationships within and between different variables.</p>
                    <p>\[C=MM^T\]</p>
                </li>

                <li>
                    <p>Calculating the eigen values and eigen vectors of the covariance matrix: the eigen vector represents the direction and the eigen value represents the magnitude of the covariance value. Therefore, principal components depict the directions of the data that having a maximum of variance.</p>
                    <p>Example of calculating eigen values and eigen vector of 2x2 matrix \(C\).</p>
                    <p>\[C = \left[\begin{array}{}c_{00}&c_{01}\\c_{10}&c_{11}\end{array} \right]\]</p>
                    <p> Assume that \(v\) is an eigenvector of the matrix \(C\); therefore it has to satisfy the following condition. </p>
                    <p>\[Cv=\lambda v\]</p>
                    <p>\[\Rightarrow (C - \lambda I)v = 0 \]</p>
                    <p>\[\Rightarrow \left[\begin{array}{}c_{00} - \lambda&c_{01}\\c_{10}&c_{11} - \lambda\end{array} \right]v = 0 \tag{1}\]</p>
                    <p>The equation \((1)\) has a non-zero solution iff the matrix \(\left[\begin{array}{}c_{00} - \lambda&c_{01}\\c_{10}&c_{11} - \lambda\end{array} \right]\) is not invertible or its determinant is zero.</p>
                    <p>\[\det = (c_{00} - \lambda)(c_{11} - \lambda) - c_{10}c_{01} = 0\]</p>
                    <p>\[\Rightarrow \lambda^2 - (c_{00} + c_{11})\lambda + c_{00}c_{11} - c_{10}c_{01} = 0 \tag{2}\]</p>
                    <p>Solve the quadratic equation \((2)\), we have \(\Delta = (c_{00} + c_{11})^2 - 4(c_{00}c_{11} - c_{10}c_{01}) = c_{00}^2 + c_{11}^2 - 2c_{00}c_{11} + 4c_{10}c_{01} = (c_{00} - c_{11})^2 + 4c_{10}c_{01}\)</p>
                    <p>\[\lambda_1 = \frac{c_{00} + c_{11} - \sqrt{\Delta}}{2}, \lambda_2 = \frac{c_{00} + c_{11} + \sqrt{\Delta}}{2}\]</p>
                    <p>Substitute \(\lambda_1\) and \(\lambda_2\) into \((1)\) and calculate eigen vectors \(v_1\) and \(v_2\) respectively. </p>
                </li>
            </ol>


			<h3>2. Experiment</h3>
			<p>Demo video</p>
			<iframe width="853" height="480" frameborder="0" allowfullscreen src="https://www.youtube.com/embed/f0N5d-CnJv4"></iframe>

			<h3>3. References</h3>
			<p>
				<ul>
                    <li>https://learnopencv.com/principal-component-analysis/</li>
                    <li>https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c</li>
                    <li>https://builtin.com/data-science/step-step-explanation-principal-component-analysis</li>
                </ul>
			</p>
            </section>
        </main>
    </div>
</body>
</html>
