<!DOCTYPE html>
<html>
    <head>
        <title>Principal Component Analysis</title>
        <link rel="stylesheet" href="../../../../index_css.css">
		<script src="../../../../index.js"></script>
		<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
		<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.1/es5/tex-mml-chtml.js"></script>
    </head>

    <body>
        <div id="header"></div>
		<script>
			Header("header");
		</script> 
        <div class="navigation">
            <div class="nav_item"><a href="https://noidh.github.io">[Home]</a></div>
        </div>

        <div class="introduction">	
			<h3>1. Principal Component Analysis</h3>
			<p>Principal Component Analysis (PCA) is an important statistic technique and widely used in Machine Learning to reduce the dimension of the dataset.</p>
            <p>In other words, PCA help us eliminate the unuseful information or noise and keep the most important once in data.</p>
            <p>Below are steps for calculating PCA in 2D data which is can be easily extended to higher dimension dataset.</p>
            <ol>
                <li>
                    <p>Creating a matrix \(D\) by assembling all the data points into it where each column is one data point.</p>
                    <p>\[D=\left[\begin{array}{} x_1 & x_2 & ... & x_n \\ y_1 & y_2 & ... & y_n \end{array} \right]\]</p>
                </li>

                <li>
                    <p>Calculating the mean of each row of matrix \(D\)</p>
                    <p>\[\mu_x = \frac{1}{n}\sum_{i=1}^n x_i\]</p>
                    <p>\[\mu_y = \frac{1}{n}\sum_{i=1}^n y_i\]</p>
                </li>

                <li>
                    <p>Creating a matrix \(M\) by subtracting it's value by the corresponding mean value: this step is to standardize the range of variables.
                        Since variables which have larger ranges potentially dominate the output of calculation and it can lead to wrong results. we can obtain a better result by dividing by the standard deviation after subtracting by the mean value.
                    </p>    
                    <p><p>\[M=\left[\begin{array}{} (x_1 - \mu_x) & (x_2 - \mu_x) & ... & (x_n - \mu_x) \\ (y_1 - \mu_y) & (y_2 - \mu_y) & ... & (y_n - \mu_y) \end{array} \right]\]</p></p>
                </li>

                <li>
                    <p>Calculating the covariance matrix \(C\): the reason why we need to calculate the covariance matrix is expoloring the relationship between variables itself and between difference variables.</p>
                    <p>\[C=MM^T\]</p>
                </li>

                <li>
                    <p>Calculating the eigen values and eigen vectors of the covariance matrix: the eigen vector represents the direction and the eigen value represents the magnitude of the covariance value. Therefore, principal components depict the directions of the data that having a maximum of variance.</p>
                    <p>Example of calculating eigen values and eigen vector of 2x2 matrix \(C\).</p>
                    <p>\[C = \left[\begin{array}{}c_{00}&c_{01}\\c_{10}&c_{11}\end{array} \right]\]</p>
                    <p> Assume that \(v\) is an eigen vector of the matrix \(C\), thererfore it has to satisfy below condition. </p>
                    <p>\[Cv=\lambda v\]</p>
                    <p>\[\Rightarrow (C - \lambda I)v = 0 \]</p>
                    <p>\[\Rightarrow \left[\begin{array}{}c_{00} - \lambda&c_{01}\\c_{10}&c_{11} - \lambda\end{array} \right]v = 0 \tag{1}\]</p>
                    <p>The equation \((1)\) has non-zero solution iff the matrix \(\left[\begin{array}{}c_{00} - \lambda&c_{01}\\c_{10}&c_{11} - \lambda\end{array} \right]\) is not invertable or its determinant is zero.</p>
                    <p>\[\det = (c_{00} - \lambda)(c_{11} - \lambda) - c_{10}c_{01} = 0\]</p>
                    <p>\[\Rightarrow \lambda^2 - (c_{00} + c_{11})\lambda + c_{00}c_{11} - c_{10}c_{01} = 0 \tag{2}\]</p>
                    <p>Solve the quadratic equation \((2)\), we have \(\Delta = (c_{00} + c_{11})^2 - 4(c_{00}c_{11} - c_{10}c_{01}) = c_{00}^2 + c_{11}^2 - 2c_{00}c_{11} + 4c_{10}c_{01} = (c_{00} - c_{11})^2 + 4c_{10}c_{01}\)</p>
                    <p>\[\lambda_1 = \frac{c_{00} + c_{11} - \sqrt{\Delta}}{2}, \lambda_2 = \frac{c_{00} + c_{11} + \sqrt{\Delta}}{2}\]</p>
                    <p>Substitute \(\lambda_1\) and \(\lambda_2\) into \((1)\) and calculate eigen vectors \(v_1\) and \(v_2\) respectively. </p>
                </li>
            </ol>


			<h3>2. Experiment</h3>
			<p>Demo video</p>
			<iframe width="853" height="480" frameborder="0" allowfullscreen src="https://www.youtube.com/embed/f0N5d-CnJv4"></iframe>

			<h3>3. References</h3>
			<p>
				<ul>
                    <li>https://learnopencv.com/principal-component-analysis/</li>
                    <li>https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c</li>
                    <li>https://builtin.com/data-science/step-step-explanation-principal-component-analysis</li>
                </ul>
			</p>
		</div>
    </body>
</html>