<!DOCTYPE html>
<html lang="en-US">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Logistic Regression â€” Doan Huu Noi</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=DM+Sans:ital,opsz,wght@0,9..40,400;0,9..40,500;0,9..40,600;0,9..40,700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../../../../index_css.css">
    <script src="../../../../index.js"></script>

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.1/es5/tex-mml-chtml.js"></script>

</head>
<body>
    <div id="header" class="site-header"></div>
    <script>Header("header");</script>

    <div class="layout">
        <aside class="sidebar" aria-label="Site navigation">
            <nav class="nav">
                <div class="nav_item">
                    <a href="../../../../index.html" class="nav_link">Home</a>
                </div>
                <div class="nav_item nav_item--dropdown">
                    <a href="../../../../project.html" class="nav_link">Personal Projects</a>
                    <ul class="nav_dropdown">
                        <li><a href="../../../../pages/sw/ximagetool/ximagetool.html">XImage Tool</a></li>
                        <li><a href="../../../../pages/sw/ximage2text/ximagetotext.html">XText</a></li>
                    </ul>
                </div>
                <div class="nav_item nav_item--dropdown">
                    <a href="../../../../blog.html" class="nav_link nav_link--current">Technical Blog</a>
                    <ul class="nav_dropdown">
                        <li><a href="../../../../notes_ip.html">Image Processing</a></li>
                        <li><a href="../../../../notes_cv.html">Computer Vision</a></li>
                        <li><a href="../../../../notes_ml.html">Machine Learning</a></li>
                        <li><a href="../../../../opengl_notes.html">3D Rendering</a></li>
                        <li><a href="../../../../notes_miscellaneous.html">Miscellaneous</a></li>
                    </ul>
                </div>
            </nav>
        </aside>

        <main class="content">
            <section class="section page-prose">
			<h3>1. Logistic Regression</h3>
			<p>Logistic regression is used to solve classification problems, especially binary classification problems, in which the model output is a discrete value corresponding to a class or category.</p>
            <p>In order to squash the input value from a wide unbounded range into a limited range \((0, 1)\), we use the sigmoid function as below.</p>
            <p>\[g(z) = \frac{1}{1 + e^{-z}}\]</p>
            <p>The sigmoid function can be visualized as the graph below</p>
            <img class="image_center"  src="sigmoid.png">

            <p>With the input feature vector \(X = [\begin{array}{}1&x_1&x_2&...&x_N\end{array}]\) and the actual value \(Y\), the objective of logistic regression is to find the weight vector \(W = [\begin{array}{}w_0&w_1&w_2&...&w_N\end{array}]\) so that it satisfies the following condition.</p>
            <p>\(W^T X\) is a linear combination output of the feature vector and the weight vector, we apply sigmoid function to get hypothesis function.</p>
            <p>\[h_{W}(X) = \frac{1}{1 + e^{-W^T X}}\]</p>
            <p>In binary classification problems, there are two possible cases</p>
            <ul>
                <li>
                    <p>The input actual value \(Y = 1\): If the output of the hypothesis function \(h_{W}(X)\) is also \(1\), the error or the output of the cost function must be zero. Otherwise, if the output of the hypothesis function \(h_{W}(X)\) is zero, the output of the cost function must be very large. In order to satisfy this case, the cost function is defined as \(-\log(h_{W}(X))\)</p>
                </li>

                <li>
                    <p>The input actual value \(Y = 0\): Similarly, the cost function is \(-\log(1 - h_{W}(X))\)</p>
                </li>
            </ul>

            <p>The cost function for 2 cases</p>
            <p>\[cost(h_{W}(X), Y) = \begin{cases}-\log(h_{W}(X)) & Y = 1 \\ -\log(1 - h_{W}(X)) & Y = 0 \end{cases}\]</p>
            <p>\[cost(h_{W}(X), Y) = -Y \times \log(h_{W}(X)) -(1 - Y) \times \log(1 - h_{W}(X)) \]</p>

            <p>For \(M\) observations, the cost function is</p>
            <p>\[J(w) = -\frac{1}{M} \sum_{i = 1}^{M}\left[ Y^i \times \log(h_{W}(X^i)) + (1 - Y^i) \times \log(1 - h_{W}(X^i)) \right]\]</p>

            <h4>Gradient Descent</h4>
            <p>To apply gradient descent to update the value of \(w_j\), we need to take the partial derivative of the cost function at each \(w_j\)</p>
            <p>\[\frac{\delta J(w)}{\delta w_j} = -\frac{1}{M} \sum_{i = 1}^{M}\left[ Y^i \times \frac{\delta \log(h_{W}(X^i))}{\delta w_j} + (1 - Y^i) \times \frac{\delta \log(1 - h_{W}(X^i))}{\delta w_j} \right] = -\frac{1}{M} \sum_{i = 1}^{M}\left[ Y^i \times \frac{\frac{\delta h_{W}(X^i)}{\delta w_j}}{h_{W}(X^i)} - (1 - Y^i) \times \frac{\frac{\delta h_{W}(X^i)}{\delta w_j}}{1 - h_{W}(X^i)} \right] \tag{1}\]</p>
            <p>We know that</p>
            <p>\[\frac{\delta h_{W}(X)}{\delta w_j} = \frac{\delta \frac{1}{1 + e^{-W^T X}}}{\delta w_j} = \frac{e^{-{W^T}X} \times x_j}{(1 + e^{-{W^T}X})^2} = \frac{1}{1 + e^{-{W^T}X}} \times \frac{1 + e^{-{W^T}X} - 1}{1 + e^{-{W^T}X}} \times x_j = \frac{1}{1 + e^{-{W^T}X}} \times (1 - \frac{1}{1 + e^{-{W^T}X}}) \times x_j = h_{W}(X) \times (1 - h_{W}(X)) \times x_j \tag{2}\]</p>

            <p>Substitute \((2)\) into \((1)\), we have</p>
            <p>\[\frac{\delta J(w)}{\delta w_j} = -\frac{1}{M} \sum_{i = 1}^{M}\left(Y^i \times (1 - h_{W}(X^i)) \times x_j^i + (1 - Y^i) \times h_{W}(X^i) \times x_j^i \right) = \frac{1}{M} \sum_{i = 1}^{M}\left(h_{W}(X^i) - Y^i\right) \times x_j^i\]</p>

            <p>The learning function is</p>
            <p>\[w_j \leftarrow w_j - \alpha \times \frac{1}{M} \sum_{i = 1}^{M}\left(h_{W}(X^i) - Y^i\right) \times x_j^i\]</p>

            <h3>2. Experiment</h3>
			<p>Demo video</p>
			<iframe width="853" height="480" frameborder="0" allowfullscreen src="https://www.youtube.com/embed/S4GNV9uLjNQ"></iframe>

			<h3>3. References</h3>
			<p>
				<ul>
                    <li>https://www.baeldung.com/cs/gradient-descent-logistic-regression</li>
                    <li>https://christophm.github.io/interpretable-ml-book/logistic.html</li>
                    <li>https://towardsdatascience.com/why-not-mse-as-a-loss-function-for-logistic-regression-589816b5e03c</li>
                    <li>https://towardsdatascience.com/multiclass-classification-algorithm-from-scratch-with-a-project-in-python-step-by-step-guide-485a83c79992</li>
                </ul>
			</p>
            </section>
        </main>
    </div>
</body>
</html>
