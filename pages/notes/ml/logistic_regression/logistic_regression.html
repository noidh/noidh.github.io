<!DOCTYPE html>
<html>
    <head>
        <title>Logistic Regression</title>
        <link rel="stylesheet" href="../../../../index_css.css">
		
		<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
		<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.1/es5/tex-mml-chtml.js"></script>
    </head>

    <body>
        <div class="header">
			<div  style="width: 270px; float: left;">
				<h2 style="display: flex; align-self: left; position:absolute; top:0px;">Doan Huu Moi (MEO)</h2>
				<h4></h4><p><a><< doanhuunoi@gmail.com >></a></p></h4>
				<p style="word-wrap: break-word;">I'm an enthusiastic researcher in Image Processing and Computer Vision. Learning new theories and bringing them to practice is my hobby. My research topics are Pattern Matching, Stereo-Vision Reconstruction, and other Miscellaneous.</p>
			</div>
			<div style="float: left;">
				<img style="height: 220px;" src="https://noidh.github.io/header.jpg">
			</div>
        </div>
        <div class="navigation">
            <div class="nav_item"><a href="https://noidh.github.io">[Home]</a></div>
        </div>

        <div class="introduction">	
			<h3>1. Logistic Regression</h3>
			<p>Logistic regression is used to solve classification problems, especially binary classification problems. in which, model's output is a discrete value corresponding to class or category.</p>
            <p>In order to squash the input value from a wide unbounded range into a limited range \((0, 1)\), we use the sigmoid function as below.</p>
            <p>\[g(z) = \frac{1}{1 + e^{-z}}\]</p>
            <p>The sigmoid function can be visualized as below graph</p>
            <img class="image_center"  src="sigmoid.png">

            <p>With the input feature vector \(X = [\begin{array}{}1&x_1&x_2&...&x_N\end{array}]\) and the actual value \(Y\), the objective of logistic regression is to find the weight vector \(W = [\begin{array}{}w_0&w_1&w_2&...&w_N\end{array}]\) so that it satisfies the following condition.</p>
            <p>\(W^T X\) is a linear combination output of the feature vector and the weight vector, we apply sigmoid function to get hypothesis function.</p>
            <p>\[h_{W}(X) = \frac{1}{1 + e^{-W^T X}}\]</p>
            <p>In binary classification problems, there are 2 cases can happen</p>
            <ul>
                <li>
                    <p>The input actual value \(Y = 1\): If the output of the hypothesis function \(h_{W}(X)\) is also \(1\), the error or the output of the cost function must be zero. Otherwise, if the output of the hypothesis function \(h_{W}(X)\) is zero, the output of the cost function must be very large. In order to satisfy this case, the cost function is defined as \(-\log(h_{W}(X))\)</p>
                </li>

                <li>
                    <p>The input actual value \(Y = 0\): Similarly, the cost function is \(-\log(1 - h_{W}(X))\)</p>
                </li>
            </ul>

            <p>The cost function for 2 cases</p>
            <p>\[cost(h_{W}(X), Y) = \begin{cases}-\log(h_{W}(X)) & Y = 1 \\ -\log(1 - h_{W}(X)) & Y = 0 \end{cases}\]</p>
            <p>\[cost(h_{W}(X), Y) = -Y \times \log(h_{W}(X)) -(1 - Y) \times \log(1 - h_{W}(X)) \]</p>

            <p>For \(M\) observations, the cost function is</p>
            <p>\[J(w) = -\frac{1}{M} \sum_{i = 1}^{M}\left[ Y^i \times \log(h_{W}(X^i)) + (1 - Y^i) \times \log(1 - h_{W}(X^i)) \right]\]</p>

            <h4>Gradient Descent</h4>
            <p>To apply gradient descent to update the value of \(w_j\), we need to take the partial derivate of the cost function at each \(w_j\)</p>
            <p>\[\frac{\delta J(w)}{\delta w_j} = -\frac{1}{M} \sum_{i = 1}^{M}\left[ Y^i \times \frac{\delta \log(h_{W}(X^i))}{\delta w_j} + (1 - Y^i) \times \frac{\delta \log(1 - h_{W}(X^i))}{\delta w_j} \right] = -\frac{1}{M} \sum_{i = 1}^{M}\left[ Y^i \times \frac{\frac{\delta h_{W}(X^i)}{\delta w_j}}{h_{W}(X^i)} - (1 - Y^i) \times \frac{\frac{\delta h_{W}(X^i)}{\delta w_j}}{1 - h_{W}(X^i)} \right] \tag{1}\]</p>
            <p>We know that</p>
            <p>\[\frac{\delta h_{W}(X)}{\delta w_j} = \frac{\delta \frac{1}{1 + e^{-W^T X}}}{\delta w_j} = \frac{e^{-{W^T}X} \times x_j}{(1 + e^{-{W^T}X})^2} = \frac{1}{1 + e^{-{W^T}X}} \times \frac{1 + e^{-{W^T}X} - 1}{1 + e^{-{W^T}X}} \times x_j = \frac{1}{1 + e^{-{W^T}X}} \times (1 - \frac{1}{1 + e^{-{W^T}X}}) \times x_j = h_{W}(X) \times (1 - h_{W}(X)) \times x_j \tag{2}\]</p>

            <p>Substitute \((2)\) into \((1)\), we have</p>
            <p>\[\frac{\delta J(w)}{\delta w_j} = -\frac{1}{M} \sum_{i = 1}^{M}\left(Y^i \times (1 - h_{W}(X^i)) \times x_j^i + (1 - Y^i) \times h_{W}(X^i) \times x_j^i \right) = \frac{1}{M} \sum_{i = 1}^{M}\left(h_{W}(X^i) - Y^i\right) \times x_j^i\]</p>

            <p>The learning function is</p>
            <p>\[w_j \leftarrow w_j - \alpha \times \frac{1}{M} \sum_{i = 1}^{M}\left(h_{W}(X^i) - Y^i\right) \times x_j^i\]</p>

            <h3>2. Experiment</h3>
			<p>Demo video</p>
			<iframe width="853" height="480" frameborder="0" allowfullscreen src="https://www.youtube.com/embed/J6FhSzfvqsU"></iframe>

			<h3>3. References</h3>
			<p>
				<ul>
                    <li>https://www.baeldung.com/cs/gradient-descent-logistic-regression</li>
                    <li>https://christophm.github.io/interpretable-ml-book/logistic.html</li>
                    <li>https://towardsdatascience.com/why-not-mse-as-a-loss-function-for-logistic-regression-589816b5e03c</li>
                    <li>https://towardsdatascience.com/multiclass-classification-algorithm-from-scratch-with-a-project-in-python-step-by-step-guide-485a83c79992</li>
                </ul>
			</p>
		</div>
    </body>
</html>