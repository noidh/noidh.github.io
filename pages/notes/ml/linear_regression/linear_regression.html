<!DOCTYPE html>
<html>
    <head>
        <title>Linear Regression</title>
        <link rel="stylesheet" href="../../../../index_css.css">
		
		<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
		<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.1/es5/tex-mml-chtml.js"></script>
    </head>

    <body>
        <div class="header">
			<div  style="width: 270px; float: left;">
				<h2 style="display: flex; align-self: left; position:absolute; top:0px;">Doan Huu Noi (NEO)</h2>
				<h4></h4><p><a><< doanhuunoi@gmail.com >></a></p></h4>
				<p style="word-wrap: break-word;">I'm an enthusiastic researcher in Image Processing and Computer Vision. Learning new theories and bringing them to practice is my hobby. My research topics are Pattern Matching, Stereo-Vision Reconstruction, and other Miscellaneous.</p>
			</div>
			<div style="float: left;">
				<img style="height: 220px;" src="https://noidh.github.io/header.jpg">
			</div>
        </div>
        <div class="navigation">
            <div class="nav_item"><a href="https://noidh.github.io">[Home]</a></div>
        </div>

        <div class="introduction">	
			<h3>1. Linear Regression</h3>
			<p>Linear regression is used for modeling the linear relationship between the input variables and the corresponding output variables. After that, with the trained model and an arbitrary input value, we can predict or forecast an approximation output value.</p>
            <p>For a simple case in 2D, the modeling process is to find parameters \(m\) and \(b\) so that the below equation is satisfied.</p>
            <p>\[y=mx + b\]</p>
            <img class="image_center" src="linear_regression.png">

            <h4>Cost Function</h4>
            <p>In order to find the best values for \(m\) and \(b\), we choose Mean Square Error (MSE) function as a cost function and minimize this cost function help us find these parameter values.</p>
			<p>\[minimize \frac{1}{N} \sum_{i=1}^N (y_i^{'} - y_i)^2\]</p>
            <p>\[J=\frac{1}{N} \sum_{i=1}^N (y_i^{'} - y_i)^2\]</p>
            <p>For linear regression, the MSE is a convex function, therefore it's guaranteed to find a global minima.</p>

            <h4>Gradient Descent</h4>
            <p>Gradient descent is an iteration way used for adjusting parameter values to make the cost function converge to minima.</p>
            <p>By taking the partial derivatices with respect to \(m\) and \(b\)</p>
            <p>\[J=\frac{1}{N} \sum_{i=1}^N (y_i^{'} - y_i)^2 =\frac{1}{N} \sum_{i=1}^N (mx_i + b - y_i)^2 \]</p>
            <p>\[\frac{\delta J}{\delta m} = \frac{2}{N} \sum_{i=1}^N (mx_i + b - y_i) \cdot x_i = \frac{2}{N} \sum_{i=1}^N (y_i^{'} - y_i) \cdot x_i\]</p>
            <p>\[\frac{\delta J}{\delta b} = \frac{2}{N} \sum_{i=1}^N (mx_i + b - y_i) = \frac{2}{N} \sum_{i=1}^N (y_i^{'} - y_i)\]</p>

            <p>The learning function is defined as</p>
            <p>\[m \leftarrow m - \alpha \cdot \frac{\delta J}{\delta m} = m - \alpha \cdot \frac{2}{N} \sum_{i=1}^N (y_i^{'} - y_i) \cdot x_i\]</p>
            <p>\[b \leftarrow b - \alpha \cdot \frac{\delta J}{\delta b} = b - \alpha \cdot \frac{2}{N} \sum_{i=1}^N (y_i^{'} - y_i) \]</p>
            <p>Where \(a\) is the learning rate. A smaller learning rate helps us get closer to minima but it's slow, a larger learning rate can make cost function converge to the minima faster but it can lead to overshoot.</p>

            <img class="image_center" src="learning_rate.png">

            <p>*) To avoid bias (the larger range feature gives more impaction to result) or overflow while calculating, the train data should be normalized or standardized into the same range before training</p>
            <p>Normalization: \(z = \frac{x - x_{min}}{x_{max} - x_{min}}\)</p>
            <p>Standardization: \(z = \frac{x - \mu}{\sigma}\)</p>
            <h3>2. Experiment</h3>
			<p>Demo video</p>
			

			<h3>3. References</h3>
			<p>
				<ul>
                    <li>https://towardsdatascience.com/introduction-to-machine-learning-algorithms-linear-regression-14c4e325882a</li>
                    <li>https://machinelearningmastery.com/linear-regression-for-machine-learning/</li>
                    <li>https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/</li>
                    <li>https://towardsai.net/p/data-science/how-when-and-why-should-you-normalize-standardize-rescale-your-data-3f083def38ff</li>
                    <li>https://www.baeldung.com/cs/normalization-vs-standardization</li>
                    <li><a href="https://www.kaggle.com/general/219378">What is bias, variance, overfitting, underfitting?</a></li>
                    <li><a href="https://www.bradthiessen.com/html5/docs/ols.pdf">Why use least square instead of least absolute distance</a></li>
                    <li><a href="https://stats.stackexchange.com/questions/313235/can-i-use-gradient-descent-for-least-absolute-deviation-regression">Can I use gradient descent for Least Absolute Deviation Regression?</a></li>
                </ul>
			</p>
		</div>
    </body>
</html>