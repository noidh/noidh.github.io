<!DOCTYPE html>
<html lang="en-US">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Linear Least Square — Doan Huu Noi</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=DM+Sans:ital,opsz,wght@0,9..40,400;0,9..40,500;0,9..40,600;0,9..40,700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../../../../index_css.css">
    <script src="../../../../index.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.1/es5/tex-mml-chtml.js"></script>

</head>
<body>
    <div id="header" class="site-header"></div>
    <script>Header("header");</script>

    <header class="site-nav" aria-label="Site navigation">
        <nav class="nav nav--horizontal">
            <div class="nav_item">
                <a href="../../../../index.html" class="nav_link">Home</a>
            </div>
            <div class="nav_item nav_item--dropdown">
                <a href="../../../../project.html" class="nav_link">Personal Projects ▾</a>
                <ul class="nav_dropdown">
                    <li><a href="../../../../pages/sw/ximagetool/ximagetool.html">XImage Tool</a></li>
                    <li><a href="../../../../pages/sw/ximage2text/ximagetotext.html">XText</a></li>
                </ul>
            </div>
            <div class="nav_item nav_item--dropdown">
                <a href="../../../../blog.html" class="nav_link nav_link--current">Technical Blog ▾</a>
                <ul class="nav_dropdown">
                    <li><a href="../../../../notes_ip.html">Image Processing</a></li>
                    <li><a href="../../../../notes_cv.html">Computer Vision</a></li>
                    <li><a href="../../../../notes_ml.html">Machine Learning</a></li>
                    <li><a href="../../../../opengl_notes.html">3D Rendering</a></li>
                    <li><a href="../../../../notes_miscellaneous.html">Miscellaneous</a></li>
                </ul>
            </div>
        </nav>
    </header>

    <div class="layout">
        <main class="content">
            <section class="section page-prose">
			<h3>1. Introduction</h3>
			<p>\(x\) is an exact solution to linear problem \(Ax=b\) if the error \(e=b - Ax\) is zero. However, when the error \(e\) isn't zero and its length is as small as possible, we can find a least squares solution \(\widehat{x}\).
			</p>	
			<p>Therefore, if \(Ax=b\) has no exact solution, we can solve \(A\widehat{x}=b\) to obtain an approximation solution.</p>
			<p>In order to solve \(A\widehat{x}=b\), multiply both sides by \(A^{T}\), the transpose of \(A\)</p>
			<p>
				\[A^{T}A\widehat{x}=A^{T}b\]
			</p>
			<p>Since \(A^{T}A\) is a square matrix, we can compute inverse matrix of it.</p>
			<p>
				\[\widehat{x}=\left[A^{T}A\right]^{-1}A^{T}b\tag{1}\]
			</p>
			<p>To apply the least squares approximation to fit a straight line in 2D, we assume that there are N 2D points \((x_{1-N}, y_{1-N})\). A straight line that has least square distance to each point can be described as \(y=Mx + B\)</p>
			<p>The linear problem \(Ax=b\) now is \(\left[\begin{array}{ccc}x_{1} & 1\\x_{2} & 1\\...\\x_{N} & 1\end{array}\right]\left[\begin{array}{ccc}M\\B\end{array}\right]=\left[\begin{array}{c}y_{1}\\y_{2}\\...\\y_{N}\end{array}\right]\)</p>
            <p>Where \(A=\left[\begin{array}{ccc}x_{1} & 1\\x_{2} & 1\\...\\x_{N} & 1\end{array}\right]\), \(x=\left[\begin{array}{ccc}M\\B\end{array}\right]\), and \(b=\left[\begin{array}{c}y_{1}\\y_{2}\\...\\y_{N}\end{array}\right]\)</p>
			<p>\[\left[A^{T}A\right]^{-1}=\left[\begin{array}{}\sum_{i}^{N}x_{i}^{2} & \sum_{i}^{N}x_{i}\\\sum_{i}^{N}x_{i}&N\end{array}\right], A^{T}b=\left[\begin{array}{}\sum_{i}^{N}x_{i}y_{i}\\\sum_{i}^{N}y_{i}\end{array}\right]\tag{2}\]</p>
			<p>From \((1)\) and \((2)\), we have \(\widehat{x}=\left[\begin{array}{}\sum_{i}^{N}x_{i}^{2} & \sum_{i}^{N}x_{i}\\\sum_{i}^{N}x_{i}&N\end{array}\right]^{-1}\left[\begin{array}{}\sum_{i}^{N}x_{i}y_{i}\\\sum_{i}^{N}y_{i}\end{array}\right]\)</p>
			<p>We need to find inverse of square matrix \(\left[\begin{array}{}\sum_{i}^{N}x_{i}^{2} & \sum_{i}^{N}x_{i}\\\sum_{i}^{N}x_{i}&N\end{array}\right]^{-1}\)</p>
			<p>\(Det=N\sum_{i}^{N}x_{i}^{2} - (\sum_{i}^{N}x_{i})^2\), check that \(Det\) is different from zero.</p>
			<p>Inverse matrix is \[\frac{1}{N\sum_{i}^{N}x_{i}^{2} - (\sum_{i}^{N}x_{i})^2}\left[\begin{array}{}N & -\sum_{i}^{N}x_{i}\\-\sum_{i}^{N}x_{i}&\sum_{i}^{N}x_{i}^{2}\end{array}\right]\tag{3}\]</p>
			
			<p>Finally, we have </p>
			<p>\[\widehat{x}=\frac{1}{N\sum_{i}^{N}x_{i}^{2} - (\sum_{i}^{N}x_{i})^2}\left[\begin{array}{}N & -\sum_{i}^{N}x_{i}\\-\sum_{i}^{N}x_{i}&\sum_{i}^{N}x_{i}^{2}\end{array}\right]\left[\begin{array}{}\sum_{i}^{N}x_{i}y_{i}\\\sum_{i}^{N}y_{i}\end{array}\right]\]</p>
			<p>Then, \(M=\frac{N\sum_{i}^{N}x_{i}y_{i} - \sum_{i}^{N}x_{i}\sum_{i}^{N}y_{i}}{N\sum_{i}^{N}x_{i}^{2} - (\sum_{i}^{N}x_{i})^2}\), \(B=\frac{\sum_{i}^{N}x_{i}^{2}\sum_{i}^{N}y_{i} - \sum_{i}^{N}x_{i}\sum_{i}^{N}x_{i}y_{i}}{N\sum_{i}^{N}x_{i}^{2} - (\sum_{i}^{N}x_{i})^2}\) or \(B=\frac{\sum_{i}^{N}y_{i} - M\sum_{i}^{N}x_{i}}{N}\)</p>
			
			<br>
			<h3>2. Experiment</h3>
			<iframe width="853" height="480" frameborder="0" allowfullscreen src="https://www.youtube.com/embed/JXKumJ3AX9U"></iframe>

		
			<h3>3. References</h3>
			<p>
				<ul>
					<li>https://en.wikipedia.org/wiki/Least_squares</li>
					<li>https://math.mit.edu/~gs/linearalgebra/ila0403.pdf</li>
				</ul>
			</p>
            </section>
        </main>
    </div>
</body>
</html>
