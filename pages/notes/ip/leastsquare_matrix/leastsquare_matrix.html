<!DOCTYPE html>
<html>
    <head>
        <title>Linear Least Square Approximation</title>
        <link rel="stylesheet" href="../../../../index_css.css">
		
		<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
		<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.1/es5/tex-mml-chtml.js"></script>
    </head>

    <body>
        <div class="header">

        </div>
        <div class="navigation">
            <div class="nav_item"><a href="https://noidh.github.io">Main</a></div>
        </div>

        <div class="introduction">	
			<h3>1. Introduction</h3>
			<p>A linear problem \(Ax=b\) often has no solution, that means the error \(e=b - Ax\) isn't always zero.
			When \(e\) is zero, \(x\) is an exact solution to \(Ax=b\). When the length of \(e\) is as small as possible, \(\widehat{x}\) is a least squares solution.
			</p>	
			<p>So, when \(Ax=b\) has no solution, we can solve \(A\widehat{x}=b\) to obtain an approximation solution.</p>
			<p>In order to solve \(A\widehat{x}=b\), multiply 2 side by \(A^{T}\) transpose matrix of \(A\)</p>
			<p>
				\[A^{T}A\widehat{x}=A^{T}b\]
			</p>
			<p>Since \(A^{T}A\) is a square matrix, we can compute inverse matrix of it.</p>
			<p>
				\[\widehat{x}=\left[A^{T}A\right]^{-1}A^{T}b\tag{1}\]
			</p>
			<p>Apply least squares approximation to fit a straight line in 2D, assume there are N pair of 2D points \((x_{1-N}, y_{1-N})\) we have equation for a line is \(Mx + B = y\)</p>
			<p>\(Ax=b\) is \(\left[\begin{array}{ccc}x_{1} & 1\\x_{2} & 1\\...\\x_{N} & 1\end{array}\right]\left[\begin{array}{ccc}M\\B\end{array}\right]=\left[\begin{array}{c}y_{1}\\y_{2}\\...\\y_{N}\end{array}\right]\)</p>
            <p>Where \(A=\left[\begin{array}{ccc}x_{1} & 1\\x_{2} & 1\\...\\x_{N} & 1\end{array}\right]\), \(x=\left[\begin{array}{ccc}M\\B\end{array}\right]\), and \(b=\left[\begin{array}{c}y_{1}\\y_{2}\\...\\y_{N}\end{array}\right]\)</p>
			<p>\[\left[A^{T}A\right]^{-1}=\left[\begin{array}{}\sum_{i}^{N}x_{i}^{2} & \sum_{i}^{N}x_{i}\\\sum_{i}^{N}x_{i}&N\end{array}\right], A^{T}b=\left[\begin{array}{}\sum_{i}^{N}x_{i}y_{i}\\\sum_{i}^{N}y_{i}\end{array}\right]\tag{2}\]</p>
			<p>From \((1)\) and \((2)\), we have \(\widehat{x}=\left[\begin{array}{}\sum_{i}^{N}x_{i}^{2} & \sum_{i}^{N}x_{i}\\\sum_{i}^{N}x_{i}&N\end{array}\right]^{-1}\left[\begin{array}{}\sum_{i}^{N}x_{i}y_{i}\\\sum_{i}^{N}y_{i}\end{array}\right]\)</p>
			<p>We need to find inverse of square matrix \(\left[\begin{array}{}\sum_{i}^{N}x_{i}^{2} & \sum_{i}^{N}x_{i}\\\sum_{i}^{N}x_{i}&N\end{array}\right]^{-1}\)</p>
			<p>\(Det=N\sum_{i}^{N}x_{i}^{2} - (\sum_{i}^{N}x_{i})^2\), check for \(Det\) is difference zero.</p>
			<p>Inverse matrix is \[\frac{1}{N\sum_{i}^{N}x_{i}^{2} - (\sum_{i}^{N}x_{i})^2}\left[\begin{array}{}N & -\sum_{i}^{N}x_{i}\\-\sum_{i}^{N}x_{i}&\sum_{i}^{N}x_{i}^{2}\end{array}\right]\tag{3}\]</p>
			
			<p>Finally, we have </p>
			<p>\[\widehat{x}=\frac{1}{N\sum_{i}^{N}x_{i}^{2} - (\sum_{i}^{N}x_{i})^2}\left[\begin{array}{}N & -\sum_{i}^{N}x_{i}\\-\sum_{i}^{N}x_{i}&\sum_{i}^{N}x_{i}^{2}\end{array}\right]\left[\begin{array}{}\sum_{i}^{N}x_{i}y_{i}\\\sum_{i}^{N}y_{i}\end{array}\right]\]</p>
			<p>Then, \(M=\frac{N\sum_{i}^{N}x_{i}y_{i} - \sum_{i}^{N}x_{i}\sum_{i}^{N}y_{i}}{N\sum_{i}^{N}x_{i}^{2} - (\sum_{i}^{N}x_{i})^2}\), \(B=\frac{\sum_{i}^{N}x_{i}^{2}\sum_{i}^{N}y_{i} - \sum_{i}^{N}x_{i}\sum_{i}^{N}x_{i}y{i}}{N\sum_{i}^{N}x_{i}^{2} - (\sum_{i}^{N}x_{i})^2}\) or \(B=\frac{\sum_{i}^{N}y_{i} - M\sum_{i}^{N}x_{i}}{N}\)</p>
			
			<br>
			<h3>2. Experiment</h3>
			
			<h3>3. References</h3>
			<p>
				<ul>
					<li>https://math.mit.edu/~gs/linearalgebra/ila0403.pdf</li>
				</ul>
			</p>
		</div>
    </body>
</html>