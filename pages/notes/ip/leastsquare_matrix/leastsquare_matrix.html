<!DOCTYPE html>
<html>
    <head>
        <title>Linear Least Square Approximation by Matrix Method</title>
        <link rel="stylesheet" href="../../../../index_css.css">
		
		<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
		<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.1/es5/tex-mml-chtml.js"></script>
    </head>

    <body>
        <div class="header">
			<div  style="width: 270px; float: left;">
				<h2 style="display: flex; align-self: left; position:absolute; top:0px;">Doan Huu Noi (NEO)</h2>
				<h4></h4><p><a><< doanhuunoi@gmail.com >></a></p></h4>
				<p style="word-wrap: break-word;">I'm an enthusiastic researcher in Image Processing and Computer Vision. Learning new theories and bringing them to practice is my hobby. My research topics are Pattern Matching, Stereo-Vision Reconstruction, and other Miscellaneous.</p>
			</div>
			<div style="float: left;">
				<img style="height: 220px;" src="https://noidh.github.io/header.jpg">
			</div>
	
        </div>
        <div class="navigation">
            <div class="nav_item"><a href="https://noidh.github.io">Main</a></div>
        </div>

        <div class="introduction">	
			<h3>1. Introduction</h3>
			<p>\(x\) is an exact solution to linear problem \(Ax=b\) if the error \(e=b - Ax\) is zero. However, when the error \(e\) isn't zero and its length is as small as possible, we can find a least squares solution \(\widehat{x}\).
			</p>	
			<p>Therefore, if \(Ax=b\) has no exact solution, we can solve \(A\widehat{x}=b\) to obtain an approximation solution.</p>
			<p>In order to solve \(A\widehat{x}=b\), multiply 2 side by \(A^{T}\) transpose matrix of \(A\)</p>
			<p>
				\[A^{T}A\widehat{x}=A^{T}b\]
			</p>
			<p>Since \(A^{T}A\) is a square matrix, we can compute inverse matrix of it.</p>
			<p>
				\[\widehat{x}=\left[A^{T}A\right]^{-1}A^{T}b\tag{1}\]
			</p>
			<p>To apply the least squares approximation to fit a straight line in 2D, we assume that there are number of N 2D points \((x_{1-N}, y_{1-N})\). A straight line that has least square distance to each point can be described as \(y=Mx + B\)</p>
			<p>The linear problem \(Ax=b\) now is \(\left[\begin{array}{ccc}x_{1} & 1\\x_{2} & 1\\...\\x_{N} & 1\end{array}\right]\left[\begin{array}{ccc}M\\B\end{array}\right]=\left[\begin{array}{c}y_{1}\\y_{2}\\...\\y_{N}\end{array}\right]\)</p>
            <p>Where \(A=\left[\begin{array}{ccc}x_{1} & 1\\x_{2} & 1\\...\\x_{N} & 1\end{array}\right]\), \(x=\left[\begin{array}{ccc}M\\B\end{array}\right]\), and \(b=\left[\begin{array}{c}y_{1}\\y_{2}\\...\\y_{N}\end{array}\right]\)</p>
			<p>\[\left[A^{T}A\right]^{-1}=\left[\begin{array}{}\sum_{i}^{N}x_{i}^{2} & \sum_{i}^{N}x_{i}\\\sum_{i}^{N}x_{i}&N\end{array}\right], A^{T}b=\left[\begin{array}{}\sum_{i}^{N}x_{i}y_{i}\\\sum_{i}^{N}y_{i}\end{array}\right]\tag{2}\]</p>
			<p>From \((1)\) and \((2)\), we have \(\widehat{x}=\left[\begin{array}{}\sum_{i}^{N}x_{i}^{2} & \sum_{i}^{N}x_{i}\\\sum_{i}^{N}x_{i}&N\end{array}\right]^{-1}\left[\begin{array}{}\sum_{i}^{N}x_{i}y_{i}\\\sum_{i}^{N}y_{i}\end{array}\right]\)</p>
			<p>We need to find inverse of square matrix \(\left[\begin{array}{}\sum_{i}^{N}x_{i}^{2} & \sum_{i}^{N}x_{i}\\\sum_{i}^{N}x_{i}&N\end{array}\right]^{-1}\)</p>
			<p>\(Det=N\sum_{i}^{N}x_{i}^{2} - (\sum_{i}^{N}x_{i})^2\), check for the \(Det\) is difference from zero.</p>
			<p>Inverse matrix is \[\frac{1}{N\sum_{i}^{N}x_{i}^{2} - (\sum_{i}^{N}x_{i})^2}\left[\begin{array}{}N & -\sum_{i}^{N}x_{i}\\-\sum_{i}^{N}x_{i}&\sum_{i}^{N}x_{i}^{2}\end{array}\right]\tag{3}\]</p>
			
			<p>Finally, we have </p>
			<p>\[\widehat{x}=\frac{1}{N\sum_{i}^{N}x_{i}^{2} - (\sum_{i}^{N}x_{i})^2}\left[\begin{array}{}N & -\sum_{i}^{N}x_{i}\\-\sum_{i}^{N}x_{i}&\sum_{i}^{N}x_{i}^{2}\end{array}\right]\left[\begin{array}{}\sum_{i}^{N}x_{i}y_{i}\\\sum_{i}^{N}y_{i}\end{array}\right]\]</p>
			<p>Then, \(M=\frac{N\sum_{i}^{N}x_{i}y_{i} - \sum_{i}^{N}x_{i}\sum_{i}^{N}y_{i}}{N\sum_{i}^{N}x_{i}^{2} - (\sum_{i}^{N}x_{i})^2}\), \(B=\frac{\sum_{i}^{N}x_{i}^{2}\sum_{i}^{N}y_{i} - \sum_{i}^{N}x_{i}\sum_{i}^{N}x_{i}y{i}}{N\sum_{i}^{N}x_{i}^{2} - (\sum_{i}^{N}x_{i})^2}\) or \(B=\frac{\sum_{i}^{N}y_{i} - M\sum_{i}^{N}x_{i}}{N}\)</p>
			
			<br>
			<h3>2. Experiment</h3>
			<iframe width="853" height="480" frameborder="0" allowfullscreen src="https://www.youtube.com/embed/JXKumJ3AX9U"></iframe>

		
			<h3>3. References</h3>
			<p>
				<ul>
					<li>https://en.wikipedia.org/wiki/Least_squares</li>
					<li>https://math.mit.edu/~gs/linearalgebra/ila0403.pdf</li>
				</ul>
			</p>
		</div>
    </body>
</html>